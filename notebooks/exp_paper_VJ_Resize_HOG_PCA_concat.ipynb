{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from skimage import data, io, feature, color, exposure\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper: Face Recognition Based on HOG and Fast PCA Algorithm\n",
    "\n",
    "    1- Viola Jones\n",
    "    2- Resize = 64x128\n",
    "    3- HOG:\n",
    "        3780 HOG features\n",
    "    4- PCA\n",
    "    5- Normalização: median normalization method (Eq 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# declare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw2 = os.path.join('..', 'Data', 'lfw2')\n",
    "def image_path(person, id_, lfw_folder = lfw2):\n",
    "    return glob(os.path.join(lfw_folder, person, '*' + id_ + '.jpg'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters: \n",
    "    - img_matrix: (ndarray)\n",
    "    - title: (string)\n",
    "Output:\n",
    "    - image plot\n",
    "'''\n",
    "def plt_img(img_matrix, title='Image', normalize=False):\n",
    "    if normalize:\n",
    "        plt.imshow(img_matrix, vmin=np.min(img_matrix), vmax=np.max(img_matrix), cmap='gray')\n",
    "    else:\n",
    "        io.imshow(img_matrix)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_two_imgs(img_a, img_b, cmap='gray', normalize=False):\n",
    "    f = plt.figure(figsize=(12, 8))\n",
    "    f.add_subplot(1,2, 1)\n",
    "    if normalize:\n",
    "        plt.imshow(img_a, vmin=np.min(img_matrix), vmax=np.max(img_matrix), cmap=cmap)\n",
    "    else:\n",
    "        plt.imshow(img_a, cmap=cmap)\n",
    "    f.add_subplot(1,2, 2)\n",
    "    if normalize:\n",
    "        plt.imshow(img_b, vmin=np.min(img_matrix), vmax=np.max(img_matrix), cmap=cmap)\n",
    "    else:\n",
    "        plt.imshow(img_b, cmap=cmap)\n",
    "    \n",
    "    plt.show(block=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters: \n",
    "- Path: The image should be in the working directory or a full path of image\n",
    "should be given;\n",
    "- color: Second argument is a flag which specifies the way image should be read.\n",
    "    cv2.IMREAD_COLOR : Loads a color image. Any transparency of image\n",
    "    will be neglected;\n",
    "    cv2.IMREAD_GRAYSCALE : Loads image in grayscale mode;\n",
    "    cv2.IMREAD_UNCHANGED : Loads image as such including alpha channel;\n",
    "Note Instead of these three flags, you can simply pass integers 1, 0 or -1\n",
    "respectively.\n",
    "Output:\n",
    "- img_array: (ndarray)\n",
    "'''\n",
    "def open_img(path, color=0):\n",
    "    return cv2.imread(path, color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Parameters:\n",
    "- path_img: A string representing the file name. The filename must include image format like .jpg, .png, etc.\n",
    "\n",
    "- img: It is the image that is to be saved (ndarray).\n",
    "\n",
    "Return Value: It returns true if image is saved successfully.\n",
    "'''\n",
    "\n",
    "def save_img(path_img, img):\n",
    "    cv2.imwrite(path_img, img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDF(path):\n",
    "    with open(path) as f:\n",
    "        file_list = f.readlines()\n",
    "    n = int(file_list[0].strip())\n",
    "    df_inicial = pd.read_csv(path, sep='\\t', skiprows=1, nrows=n, names=['pair_name_1', 'pair_id_1', 'pair_id_2'])\n",
    "    df_inicial['pair_name_2'] = None\n",
    "    df_secondary = pd.read_csv(path, sep='\\t', skiprows=n+1, names=['pair_name_1', 'pair_id_1', 'pair_name_2', 'pair_id_2'])\n",
    "    df = pd.concat([df_inicial, df_secondary])\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_batch(df, show=False, limit=np.inf):\n",
    "    for index, row in df.iterrows():\n",
    "        plt_img(open_img(row['path_pair_id_1'], color=0), title=os.path.split(row['path_pair_id_1'])[-1].split('.')[0])\n",
    "        plt_img(open_img(row['path_pair_id_2'], color=0), title=os.path.split(row['path_pair_id_2'])[-1].split('.')[0])\n",
    "        if limit == index + 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_folder = os.path.abspath('..\\\\data\\\\')\n",
    "data_folder = os.path.join('..', 'Data')\n",
    "train_path = Path(data_folder, 'pairsDevTrain.txt')\n",
    "test_path = Path(data_folder, 'pairsDevTest.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 4)\n",
      "(1000, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df_train = getDF(train_path)\n",
    "df_test = getDF(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id_1</th>\n",
       "      <th>pair_id_2</th>\n",
       "      <th>pair_name_1</th>\n",
       "      <th>pair_name_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaron_Peirsol</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Aaron_Peirsol</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id_1  pair_id_2    pair_name_1 pair_name_2\n",
       "0          1          2  Aaron_Peirsol        None\n",
       "1          3          4  Aaron_Peirsol        None"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id_1</th>\n",
       "      <th>pair_id_2</th>\n",
       "      <th>pair_name_1</th>\n",
       "      <th>pair_name_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>Abdullah_Gul</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id_1  pair_id_2   pair_name_1 pair_name_2\n",
       "0         13         14  Abdullah_Gul        None\n",
       "1         13         16  Abdullah_Gul        None"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['path_pair_id_1'] = df_train.apply(lambda x: image_path(person=x['pair_name_1'], id_= str(x['pair_id_1'])), axis=1)\n",
    "df_train['path_pair_id_2'] = df_train.apply(lambda x: image_path(person=x['pair_name_1'], id_= str(x['pair_id_2'])) if x['pair_name_2']==None \n",
    "                                            else image_path(person=x['pair_name_2'], id_= str(x['pair_id_2'])), axis=1)\n",
    "\n",
    "df_test['path_pair_id_1'] = df_test.apply(lambda x: image_path(person=x['pair_name_1'], id_= str(x['pair_id_1'])), axis=1)\n",
    "df_test['path_pair_id_2'] = df_test.apply(lambda x: image_path(person=x['pair_name_1'], id_= str(x['pair_id_2'])) if x['pair_name_2']==None \n",
    "                                          else image_path(person=x['pair_name_2'], id_= str(x['pair_id_2'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viola Jones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rectangle(detected_faces, image, title='Image', cmap_type='gray', kwargs={'lw': 20.}):\n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1)\n",
    "    # Display the image\n",
    "    ax.imshow(image, cmap=cmap_type)\n",
    "    plt.title(title)\n",
    "    for (column, row, width, height) in detected_faces:\n",
    "        rect = Rectangle(\n",
    "                (column, row),\n",
    "                width = width,\n",
    "                height = height,\n",
    "                fill=False,\n",
    "                edgecolor='r',\n",
    "                \n",
    "                )\n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)\n",
    "#     plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(original_image, column, row, width, height):\n",
    "    # the goal is crop the biggest area\n",
    "    return original_image[row:row+height, column:column + width]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the classifier and create a cascade object for face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascade_path = os.path.join('..', 'haarcascades', 'haarcascade_frontalface_alt.xml')\n",
    "face_cascade = cv2.CascadeClassifier(cascade_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: what image to use?\n",
    "### Response: Use the biggest area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_biggest_area(original_image, detected_faces):\n",
    "    \n",
    "    # the goal is crop the biggest area\n",
    "    if len(detected_faces) == 0: # viola jones didnt recognize any face\n",
    "        return original_image, (None, None, original_image.shape[0], original_image.shape[1])\n",
    "    else:\n",
    "        # detected_faces returns: column, row, width, height\n",
    "        # So, assuming all width == height\n",
    "        # get np.argmax of height\n",
    "        id_max_max_width = np.argmax(detected_faces[:, -1])\n",
    "        column, row, width, height = detected_faces[id_max_max_width]\n",
    "        return crop_image(original_image, column, row, width, height), (column, row, width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: images with differents shapes\n",
    "### Response: Use the resize methods. So, cropped all images, then used resize methods to get a standard shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update df_train and df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['path_pair_id_1_cropped'] = df_train['path_pair_id_1'].apply(lambda x: x.replace('lfw2', 'lfw2_cropped'))\n",
    "_ = df_train['path_pair_id_1_cropped'].apply(lambda x: None if os.path.isdir(os.path.split(x)[0]) else os.mkdir(os.path.split(x)[0]))\n",
    "\n",
    "df_train['path_pair_id_2_cropped'] = df_train['path_pair_id_2'].apply(lambda x: x.replace('lfw2', 'lfw2_cropped'))\n",
    "_ = df_train['path_pair_id_2_cropped'].apply(lambda x: None if os.path.isdir(os.path.split(x)[0]) else os.mkdir(os.path.split(x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['path_pair_id_1_cropped'] = df_test['path_pair_id_1'].apply(lambda x: x.replace('lfw2', 'lfw2_cropped'))\n",
    "_ = df_test['path_pair_id_1_cropped'].apply(lambda x: None if os.path.isdir(os.path.split(x)[0]) else os.mkdir(os.path.split(x)[0]))\n",
    "\n",
    "df_test['path_pair_id_2_cropped'] = df_test['path_pair_id_2'].apply(lambda x: x.replace('lfw2', 'lfw2_cropped'))\n",
    "_ = df_test['path_pair_id_2_cropped'].apply(lambda x: None if os.path.isdir(os.path.split(x)[0]) else os.mkdir(os.path.split(x)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper: Face Recognition Based on HOG and Fast PCA Algorithm\n",
    "\n",
    "    1- Viola Jones\n",
    "    2- Resize = 64x128\n",
    "    3- HOG:\n",
    "        3780 HOG features\n",
    "    4- PCA\n",
    "    5- Normalização: median normalization method (Eq 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "    1- Viola Jones\n",
    "    2- Resize = 64x128\n",
    "    3- HOG:\n",
    "        3.1- Apply Hog in all images: all_images_data_train (path_pair_id_1_cropped and path_pair_id_1_cropped) concatenat by rows without duplicates -> result n x 3780\n",
    "        3.2- Apply Hog for each pair: data_train[path_pair_id_1_cropped] and data_train[path_pair_id_2_cropped] -> result tow matrices: 2200 x 3780 \n",
    "        3.3- Apply Hog for each pair: data_test[path_pair_id_1_cropped] and data_test[path_pair_id_2_cropped] -> result tow matrices: 1000 x 3780 \n",
    "    4- PCA\n",
    "        - 4.1: Fit pca in data step 3.1\n",
    "        - 4.2:Transform data_train[path_pair_id_1_cropped]\n",
    "        - 4.3:Transform data_train[path_pair_id_2_cropped]\n",
    "        - 4.4:Transform data_test[path_pair_id_1_cropped]\n",
    "        - 4.5:Transform data_test[path_pair_id_2_cropped]\n",
    "    5- Append:\n",
    "        - Append by columns 4.2 and 4.3\n",
    "        - Append by columns 4.4 and 4.5\n",
    "    \n",
    "    6- Normalização: median normalization method (Eq 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path_image, path_to_save, dim=(100, 100)):\n",
    "    original_image = open_img(path_image, color=0)\n",
    "    grayscale_image = original_image.copy()\n",
    "    detected_faces = face_cascade.detectMultiScale(grayscale_image)# step 1\n",
    "    cropped_image, (column, row, width, height) = crop_biggest_area(original_image, detected_faces)\n",
    "    resized = cv2.resize(cropped_image, dim, interpolation = cv2.INTER_AREA) #step 2\n",
    "    save_img(path_img=path_to_save, img=resized)\n",
    "    return (column, row, width, height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get dimensions VJ and apply pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['VJ_pair_id_1'] = df_train.apply(lambda x: preprocessing(path_image=x['path_pair_id_1'], path_to_save=x['path_pair_id_1_cropped'], dim=(64,128)), axis=1)\n",
    "df_train['VJ_pair_id_2'] = df_train.apply(lambda x: preprocessing(path_image=x['path_pair_id_2'], path_to_save=x['path_pair_id_2_cropped'], dim=(64,128)), axis=1)\n",
    "\n",
    "df_test['VJ_pair_id_1'] = df_test.apply(lambda x: preprocessing(path_image=x['path_pair_id_1'], path_to_save=x['path_pair_id_1_cropped'], dim=(64,128)), axis=1)\n",
    "df_test['VJ_pair_id_2'] = df_test.apply(lambda x: preprocessing(path_image=x['path_pair_id_2'], path_to_save=x['path_pair_id_2_cropped'], dim=(64,128)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = open_img(df_train.loc[0, 'path_pair_id_1_cropped'])\n",
    "print(test_img.shape)\n",
    "plt_img(test_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: apply HOG in all images - without duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: 3.1 - Apply Hog in all images: all_images_data_train (path_pair_id_1_cropped and path_pair_id_1_cropped) concatenat by rows without duplicates -> result 3443 x 3780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1 = df_train.path_pair_id_1_cropped\n",
    "df_train_2 = df_train.path_pair_id_2_cropped\n",
    "df_train_all_images_unique = pd.concat([df_train_1, df_train_2]).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all_images_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all_images_unique_HOG = np.array([feature.hog(open_img(img), orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                                  block_norm='L2', visualize=False, transform_sqrt=False, feature_vector=True, \n",
    "                                  multichannel=False)\n",
    "                      for img in df_train_all_images_unique])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_all_images_unique_HOG.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: 3.2- Apply Hog for each pair: data_train[path_pair_id_1_cropped] and data_train[path_pair_id_2_cropped] -> result tow matrices: 2200 x 3780 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1_HOG = np.array([feature.hog(open_img(img_1), orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                                  block_norm='L2', visualize=False, transform_sqrt=False, feature_vector=True, \n",
    "                                  multichannel=False)\n",
    "                      for img_1 in df_train.loc[:,'path_pair_id_1_cropped'].values])\n",
    "\n",
    "print(X_train_1_HOG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2_HOG = np.array([feature.hog(open_img(img_2), orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                                  block_norm='L2', visualize=False, transform_sqrt=False, feature_vector=True, \n",
    "                                  multichannel=False)\n",
    "                      for img_2 in df_train.loc[:,'path_pair_id_2_cropped'].values])\n",
    "print(X_train_2_HOG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([[1 if par==None else 0 for par in df_train.loc[:,'pair_name_2'].values]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step: 3.3- Apply Hog for each pair: data_test[path_pair_id_1_cropped] and data_test[path_pair_id_2_cropped] -> result tow matrices: 1000 x 3780 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1_HOG = np.array([feature.hog(open_img(img_1), orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                                  block_norm='L2', visualize=False, transform_sqrt=False, feature_vector=True, \n",
    "                                  multichannel=False)\n",
    "                      for img_1 in df_test.loc[:,'path_pair_id_1_cropped'].values])\n",
    "\n",
    "print(X_test_1_HOG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_2_HOG= np.array([feature.hog(open_img(img_2), orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                                  block_norm='L2', visualize=False, transform_sqrt=False, feature_vector=True, \n",
    "                                  multichannel=False)\n",
    "                      for img_2 in df_test.loc[:,'path_pair_id_2_cropped'].values])\n",
    "\n",
    "print(X_test_2_HOG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array([[1 if par==None else 0 for par in df_test.loc[:,'pair_name_2'].values]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Fit pca in data step 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(df_train_all_images_unique_HOG)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=300).fit(df_train_all_images_unique_HOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step: 4.2:Transform data_train[path_pair_id_1_cropped]\n",
    "#### Step: 4.3:Transform data_train[path_pair_id_2_cropped]\n",
    "#### Step: 4.4:Transform data_test[path_pair_id_1_cropped]\n",
    "#### Step: 4.5:Transform data_test[path_pair_id_2_cropped]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1_HOG_PCA = pca.transform(X_train_1_HOG)\n",
    "X_train_1_HOG_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2_HOG_PCA = pca.transform(X_train_2_HOG)\n",
    "X_train_2_HOG_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_1_HOG_PCA = pca.transform(X_test_1_HOG)\n",
    "X_test_1_HOG_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_2_HOG_PCA = pca.transform(X_test_2_HOG)\n",
    "X_test_2_HOG_PCA.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Append:\n",
    "    - Append by columns 4.2 and 4.3\n",
    "    - Append by columns 4.4 and 4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_HOG_PCA = np.append(X_train_1_HOG_PCA, X_train_2_HOG_PCA, axis=1)\n",
    "print(X_train_HOG_PCA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_HOG_PCA = np.append(X_test_1_HOG_PCA, X_test_2_HOG_PCA, axis=1)\n",
    "print(X_test_HOG_PCA.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(y_train.shape[0])\n",
    "np.random.shuffle(arr)\n",
    "X = X_train_PCA[arr]\n",
    "y_d = y_train[arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y_d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1, 5], 'degree': np.arange(1, 5, 2), 'kernel': ['poly']}\n",
    " ]\n",
    "grid = GridSearchCV(SVC(), scoring='accuracy', n_jobs=-1, param_grid=param_grid, verbose=10)\n",
    "grid.fit(X=X_train_HOG_PCA, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best.fit(X=X_train_HOG_PCA, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = clf_best.predict(X_test_HOG_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_test, y_pred=y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver': ['sgd'], 'max_iter': [1000, 2000, 5000], \n",
    "              'alpha': 10.0 ** -np.arange(2, 5), 'hidden_layer_sizes':np.arange(10, 100, 5), 'activation':['logistic']}\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1, verbose=10, scoring='accuracy')\n",
    "clf.fit(X=X_train_HOG_PCA, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf_ = clf.best_estimator_\n",
    "best_clf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf_.fit(X=X_train_HOG_PCA, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_ = best_clf_.predict(X_test_HOG_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_true=y_test, y_pred=y_pred_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
